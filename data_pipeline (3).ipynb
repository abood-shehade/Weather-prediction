{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkhGRzcdwF0H"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAYmt7guzh8i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATiHkbo5wFds"
      },
      "source": [
        "# Reading and cleaning pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCHMmzPLy0XM"
      },
      "outputs": [],
      "source": [
        "def read_data(path):\n",
        "    dtype_dict = {\n",
        "        'Station': 'string',\n",
        "        'Date/Time': 'string',\n",
        "        'Air Dew Point': 'float',\n",
        "        'Air Temperature (OC)': 'float',\n",
        "        'Humidity %': 'float',\n",
        "        'Atmospheric Pressure': 'float',\n",
        "        'Liquid Precipitation': 'float',\n",
        "        'Manual Present Weather …': 'string',\n",
        "        'Cloud Type': 'string',\n",
        "        'Clouds Cover (Okta)': 'float',\n",
        "        'Cloud Cover %': 'float',\n",
        "        'Snow Depth.depth In CM': 'float',\n",
        "        'Horizontal Visibility In m.': 'float',\n",
        "        'Wind Direction (Degrees)': 'float',\n",
        "        'Wind Speed (MPS)': 'float',\n",
        "        'Wind Type': 'string',\n",
        "        'Wind Gust speed': 'float',\n",
        "    }\n",
        "\n",
        "    # Read the Excel file\n",
        "    df = pd.read_excel(\n",
        "        path,\n",
        "        skiprows=5,\n",
        "        dtype=dtype_dict,\n",
        "        na_values=['Null', 'N/A', '--', 'sky obscured or cloud amount cannot be estimated']\n",
        "    )\n",
        "\n",
        "    # Standardize column names\n",
        "    if 'liquid Precipitation depth In MM' in df.columns:\n",
        "        df.rename(columns={'liquid Precipitation depth In MM': 'Liquid Precipitation'}, inplace=True)\n",
        "    elif 'Liquid Precipitation' in df.columns:\n",
        "        df.rename(columns={'Liquid Precipitation': 'Liquid Precipitation'}, inplace=True)  # Ensure consistent name\n",
        "    if 'Snow Depth.depth In CM' in df.columns:\n",
        "        df.rename(columns={'Snow Depth.depth In CM': 'Snow Depth'}, inplace=True)\n",
        "    elif 'Snow Depth' in df.columns:\n",
        "        df.rename(columns={'Snow Depth': 'Snow Depth'}, inplace=True)  # Ensure consistent name\n",
        "    if 'Manual Present Weather …' in df.columns:\n",
        "        df.rename(columns={'Manual Present Weather …': 'Manual Present Weather'}, inplace=True)\n",
        "    elif 'Snow Depth' in df.columns:\n",
        "        df.rename(columns={'Manual Present Weather': 'Manual Present Weather'}, inplace=True)  # Ensure consistent name\n",
        "\n",
        "    # Convert the 'Date/Time' column to datetime format\n",
        "    df['Date/Time'] = pd.to_datetime(df['Date/Time'], errors='coerce')\n",
        "\n",
        "\n",
        "    # Set 'Date/Time' as the index for time series operations\n",
        "    df.set_index('Date/Time', inplace=True)\n",
        "    df = df.sort_index()\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0xDzD5B_R9E"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "#Final cleaning fucntion\n",
        "def clean_data(df):\n",
        "    final_df = df.copy()\n",
        "\n",
        "    # Fill NaN values in the index (assuming it's datetime or numeric)\n",
        "    final_df.index = pd.Series(final_df.index).fillna(method='ffill').values\n",
        "\n",
        "    # Columns to interpolate\n",
        "    columns_to_interpolate = [\n",
        "        'Humidity %',\n",
        "        'Air Dew Point',\n",
        "        'Atmospheric Pressure',\n",
        "        'Air Temperature (OC)',\n",
        "        'Horizontal Visibility In m.',\n",
        "        'Wind Direction (Degrees)',\n",
        "        'Clouds Cover (Okta)',\n",
        "        'Wind Speed (MPS)',\n",
        "        'Cloud Cover %'\n",
        "    ]\n",
        "\n",
        "    # Interpolate specified columns\n",
        "    for column in columns_to_interpolate:\n",
        "        if column in final_df.columns:\n",
        "            final_df[column] = final_df[column].interpolate(method='time')\n",
        "\n",
        "    # Fill NaN values with 0 in specific columns\n",
        "\n",
        "    final_df['Liquid Precipitation'] = final_df['Liquid Precipitation'].fillna(0)\n",
        "    final_df = final_df.sort_index()\n",
        "    final_df['Snow Depth'] = final_df['Snow Depth'].fillna(0)\n",
        "    final_df = final_df.drop(columns = ['Manual Present Weather','Cloud Type','Wind Type','Station','Clouds Cover (Okta)'], axis = 1)\n",
        "\n",
        "    return final_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "6VYbydwdQQaJ",
        "outputId": "0fceaf28-dec4-475a-995e-4e680196e950"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef clean_data(df):\\n    final_df = df.copy()\\n    # Reset the index to handle duplicates\\n    final_df = final_df.reset_index(drop=True)\\n\\n    # Fill NaN values in the index (assuming it\\'s datetime or numeric)\\n    final_df.index = pd.Series(final_df.index).fillna(method=\\'ffill\\').values\\n\\n    # Columns to interpolate\\n    columns_to_interpolate = [\\n        \\'Humidity %\\',\\n        \\'Air Dew Point\\',\\n        \\'Atmospheric Pressure\\',\\n        \\'Air Temperature (OC)\\',\\n        \\'Horizontal Visibility In m.\\',\\n        \\'Wind Direction (Degrees)\\',\\n        \\'Clouds Cover (Okta)\\',\\n        \\'Wind Speed (MPS)\\',\\n        \\'Cloud Cover %\\'\\n    ]\\n\\n    # Interpolate specified columns\\n    for column in columns_to_interpolate:\\n        if column in final_df.columns:\\n            # Changed from \\'time\\' to \\'linear\\'\\n            final_df[column] = final_df[column].interpolate(method=\\'linear\\')\\n\\n    # Fill NaN values with 0 in specific columns\\n\\n    final_df[\\'Liquid Precipitation\\'] = final_df[\\'Liquid Precipitation\\'].fillna(0)\\n    final_df = final_df.sort_index()\\n    final_df[\\'Snow Depth\\'] = final_df[\\'Snow Depth\\'].fillna(0)\\n    # Exclude \"Cloud Type\" and columns with excessive missing values\\n    excluded_columns = [\"Cloud Type\", \"Manual Present Weather\"]\\n    features = final_df.drop(columns=excluded_columns)\\n\\n    # Encode categorical columns\\n    categorical_cols = features.select_dtypes(include=[\"string\"]).columns\\n    le_dict = {}\\n    for col in categorical_cols:\\n        le = LabelEncoder()\\n        features[col] = le.fit_transform(features[col].fillna(\"Unknown\"))\\n        le_dict[col] = le\\n\\n    # Fill missing numerical values with mean\\n    features = features.fillna(features.mean())\\n\\n    # --- Domain Knowledge-Based Noise Detection ---\\n    noisy_indices = set()  # Using a set to avoid duplicates\\n\\n    # Rule 1: Impossible Values\\n    if \\'Humidity %\\' in final_df.columns:\\n        noisy_indices.update(final_df.index[final_df[\\'Humidity %\\'] < 0])\\n        noisy_indices.update(final_df.index[final_df[\\'Humidity %\\'] > 100])\\n    if \\'Cloud Cover %\\' in final_df.columns:\\n        noisy_indices.update(final_df.index[final_df[\\'Cloud Cover %\\'] < 0])\\n        noisy_indices.update(final_df.index[final_df[\\'Cloud Cover %\\'] > 100])\\n    if \\'Clouds Cover (Okta)\\' in final_df.columns:\\n        noisy_indices.update(final_df.index[final_df[\\'Clouds Cover (Okta)\\'] < 0])\\n        noisy_indices.update(final_df.index[final_df[\\'Clouds Cover (Okta)\\'] > 8])\\n    if \\'Air Temperature (OC)\\' in final_df.columns:\\n        noisy_indices.update(final_df.index[final_df[\\'Air Temperature (OC)\\'] < -15]) #Based on the lowest temperature ever recorded in jordan.\\n        noisy_indices.update(final_df.index[final_df[\\'Air Temperature (OC)\\'] > 55]) #Based on the highest temperature ever recorded in jordan.\\n    # Rule 2: Sudden Jumps (using a simplified example - could be more sophisticated)\\n    if \\'Air Temperature (OC)\\' in final_df.columns:\\n      temp_diff = final_df[\\'Air Temperature (OC)\\'].diff().abs()\\n      noisy_indices.update(temp_diff[temp_diff > 20].index)  # More than 20 degrees change in one interval\\n\\n    # Rule 3: Unusual Combinations (example)\\n    if \\'Humidity %\\' in final_df.columns and \\'Air Temperature (OC)\\' in final_df.columns:\\n      noisy_indices.update(final_df.index[(final_df[\\'Humidity %\\'] > 95) & (final_df[\\'Air Temperature (OC)\\'] < -10)])\\n    # --- End of Domain Knowledge-Based Noise Detection ---\\n\\n    noisy_indices = list(noisy_indices)\\n    # Loop over the columns to replace the noisy data\\n    for target_column in columns_to_interpolate:\\n      # Create target variable\\n      target = final_df[target_column]\\n\\n      # Split the data in train and test based on the indices.\\n      X_train, X_test, y_train, y_test = train_test_split(features.drop(columns=[target_column]), target, test_size=0.2, random_state=42, shuffle=False)\\n\\n      # Train the model.\\n      rf_reg = RandomForestRegressor(random_state=42, n_jobs=-1)\\n      rf_reg.fit(X_train, y_train)\\n\\n      # Make predictions only on the noisy data\\n      predicted_values = rf_reg.predict(features.loc[noisy_indices].drop(columns=[target_column]))\\n\\n      # Replace the noisy values with the predicted values\\n      final_df.loc[noisy_indices, target_column] = predicted_values\\n\\n    # Impute all of the values of the Manual Present Weather column.\\n    # Exclude \"Cloud Type\" and columns with excessive missing values\\n    excluded_columns = [\"Cloud Type\", \"Unnamed: 17\", \"Wind Gust speed\"]\\n    features = final_df.drop(columns=excluded_columns + [\"Manual Present Weather\"])\\n\\n    # Encode categorical columns\\n    categorical_cols = features.select_dtypes(include=[\"string\"]).columns\\n    le_dict = {}\\n    for col in categorical_cols:\\n        le = LabelEncoder()\\n        features[col] = le.fit_transform(features[col].fillna(\"Unknown\"))\\n        le_dict[col] = le\\n\\n    # Fill missing numerical values with mean\\n    features = features.fillna(features.mean())\\n\\n    # Extract labeled data\\n    labeled_data = final_df[final_df[\"Manual Present Weather\"].notnull()]\\n    X = features.loc[labeled_data.index]\\n    y = labeled_data[\"Manual Present Weather\"]\\n\\n    # Encode the target variable\\n    le_target = LabelEncoder()\\n    y = le_target.fit_transform(y)\\n\\n    # Train-test split\\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    # Train Random Forest Classifier\\n    rf = RandomForestClassifier(random_state=42, n_jobs=-1)\\n    rf.fit(X_train, y_train)\\n\\n    # Validate the model\\n    y_pred = rf.predict(X_val)\\n    target_names = le_target.classes_  # Extract only the classes present in y\\n    #print(classification_report(y_val, y_pred, target_names=target_names))\\n\\n    # Impute all of the values\\n    all_data = final_df\\n    X_all = features.loc[all_data.index]\\n    final_df.loc[all_data.index, \"Manual Present Weather\"] = le_target.inverse_transform(rf.predict(X_all))\\n    return final_df'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "#Not Feasable\n",
        "\"\"\"\n",
        "def clean_data(df):\n",
        "    final_df = df.copy()\n",
        "    # Reset the index to handle duplicates\n",
        "    final_df = final_df.reset_index(drop=True)\n",
        "\n",
        "    # Fill NaN values in the index (assuming it's datetime or numeric)\n",
        "    final_df.index = pd.Series(final_df.index).fillna(method='ffill').values\n",
        "\n",
        "    # Columns to interpolate\n",
        "    columns_to_interpolate = [\n",
        "        'Humidity %',\n",
        "        'Air Dew Point',\n",
        "        'Atmospheric Pressure',\n",
        "        'Air Temperature (OC)',\n",
        "        'Horizontal Visibility In m.',\n",
        "        'Wind Direction (Degrees)',\n",
        "        'Clouds Cover (Okta)',\n",
        "        'Wind Speed (MPS)',\n",
        "        'Cloud Cover %'\n",
        "    ]\n",
        "\n",
        "    # Interpolate specified columns\n",
        "    for column in columns_to_interpolate:\n",
        "        if column in final_df.columns:\n",
        "            # Changed from 'time' to 'linear'\n",
        "            final_df[column] = final_df[column].interpolate(method='linear')\n",
        "\n",
        "    # Fill NaN values with 0 in specific columns\n",
        "\n",
        "    final_df['Liquid Precipitation'] = final_df['Liquid Precipitation'].fillna(0)\n",
        "    final_df = final_df.sort_index()\n",
        "    final_df['Snow Depth'] = final_df['Snow Depth'].fillna(0)\n",
        "    # Exclude \"Cloud Type\" and columns with excessive missing values\n",
        "    excluded_columns = [\"Cloud Type\", \"Manual Present Weather\"]\n",
        "    features = final_df.drop(columns=excluded_columns)\n",
        "\n",
        "    # Encode categorical columns\n",
        "    categorical_cols = features.select_dtypes(include=[\"string\"]).columns\n",
        "    le_dict = {}\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        features[col] = le.fit_transform(features[col].fillna(\"Unknown\"))\n",
        "        le_dict[col] = le\n",
        "\n",
        "    # Fill missing numerical values with mean\n",
        "    features = features.fillna(features.mean())\n",
        "\n",
        "    # --- Domain Knowledge-Based Noise Detection ---\n",
        "    noisy_indices = set()  # Using a set to avoid duplicates\n",
        "\n",
        "    # Rule 1: Impossible Values\n",
        "    if 'Humidity %' in final_df.columns:\n",
        "        noisy_indices.update(final_df.index[final_df['Humidity %'] < 0])\n",
        "        noisy_indices.update(final_df.index[final_df['Humidity %'] > 100])\n",
        "    if 'Cloud Cover %' in final_df.columns:\n",
        "        noisy_indices.update(final_df.index[final_df['Cloud Cover %'] < 0])\n",
        "        noisy_indices.update(final_df.index[final_df['Cloud Cover %'] > 100])\n",
        "    if 'Clouds Cover (Okta)' in final_df.columns:\n",
        "        noisy_indices.update(final_df.index[final_df['Clouds Cover (Okta)'] < 0])\n",
        "        noisy_indices.update(final_df.index[final_df['Clouds Cover (Okta)'] > 8])\n",
        "    if 'Air Temperature (OC)' in final_df.columns:\n",
        "        noisy_indices.update(final_df.index[final_df['Air Temperature (OC)'] < -15]) #Based on the lowest temperature ever recorded in jordan.\n",
        "        noisy_indices.update(final_df.index[final_df['Air Temperature (OC)'] > 55]) #Based on the highest temperature ever recorded in jordan.\n",
        "    # Rule 2: Sudden Jumps (using a simplified example - could be more sophisticated)\n",
        "    if 'Air Temperature (OC)' in final_df.columns:\n",
        "      temp_diff = final_df['Air Temperature (OC)'].diff().abs()\n",
        "      noisy_indices.update(temp_diff[temp_diff > 20].index)  # More than 20 degrees change in one interval\n",
        "\n",
        "    # Rule 3: Unusual Combinations (example)\n",
        "    if 'Humidity %' in final_df.columns and 'Air Temperature (OC)' in final_df.columns:\n",
        "      noisy_indices.update(final_df.index[(final_df['Humidity %'] > 95) & (final_df['Air Temperature (OC)'] < -10)])\n",
        "    # --- End of Domain Knowledge-Based Noise Detection ---\n",
        "\n",
        "    noisy_indices = list(noisy_indices)\n",
        "    # Loop over the columns to replace the noisy data\n",
        "    for target_column in columns_to_interpolate:\n",
        "      # Create target variable\n",
        "      target = final_df[target_column]\n",
        "\n",
        "      # Split the data in train and test based on the indices.\n",
        "      X_train, X_test, y_train, y_test = train_test_split(features.drop(columns=[target_column]), target, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "      # Train the model.\n",
        "      rf_reg = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "      rf_reg.fit(X_train, y_train)\n",
        "\n",
        "      # Make predictions only on the noisy data\n",
        "      predicted_values = rf_reg.predict(features.loc[noisy_indices].drop(columns=[target_column]))\n",
        "\n",
        "      # Replace the noisy values with the predicted values\n",
        "      final_df.loc[noisy_indices, target_column] = predicted_values\n",
        "\n",
        "    # Impute all of the values of the Manual Present Weather column.\n",
        "    # Exclude \"Cloud Type\" and columns with excessive missing values\n",
        "    excluded_columns = [\"Cloud Type\", \"Unnamed: 17\", \"Wind Gust speed\"]\n",
        "    features = final_df.drop(columns=excluded_columns + [\"Manual Present Weather\"])\n",
        "\n",
        "    # Encode categorical columns\n",
        "    categorical_cols = features.select_dtypes(include=[\"string\"]).columns\n",
        "    le_dict = {}\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        features[col] = le.fit_transform(features[col].fillna(\"Unknown\"))\n",
        "        le_dict[col] = le\n",
        "\n",
        "    # Fill missing numerical values with mean\n",
        "    features = features.fillna(features.mean())\n",
        "\n",
        "    # Extract labeled data\n",
        "    labeled_data = final_df[final_df[\"Manual Present Weather\"].notnull()]\n",
        "    X = features.loc[labeled_data.index]\n",
        "    y = labeled_data[\"Manual Present Weather\"]\n",
        "\n",
        "    # Encode the target variable\n",
        "    le_target = LabelEncoder()\n",
        "    y = le_target.fit_transform(y)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Validate the model\n",
        "    y_pred = rf.predict(X_val)\n",
        "    target_names = le_target.classes_  # Extract only the classes present in y\n",
        "    #print(classification_report(y_val, y_pred, target_names=target_names))\n",
        "\n",
        "    # Impute all of the values\n",
        "    all_data = final_df\n",
        "    X_all = features.loc[all_data.index]\n",
        "    final_df.loc[all_data.index, \"Manual Present Weather\"] = le_target.inverse_transform(rf.predict(X_all))\n",
        "    return final_df\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ivm_B8mwOg-"
      },
      "source": [
        "# Testing on the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01zQgpgXcCS1",
        "outputId": "fe4df651-9457-4379-9a72-eeba4bb24cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1N8vvdzzFey"
      },
      "outputs": [],
      "source": [
        "files = [\n",
        "    'Aqaba Airport  01-07-1961 ---  30-05-2024.xlsx',\n",
        "    'Ghor El Safi  01-07-1983  ---  31-12-2023.xlsx',\n",
        "    'Irwaished  17-01-1963  --- 31-10-2021.xlsx',\n",
        "    \"Ma'an   01-07-1961  ---   30-05-2024.xlsx\",\n",
        "    \"Mafraq   01-03-1953  -  30-05-2024.xlsx\",\n",
        "    \"Queen Alia Airport   01-07-1983 - 30-05-2024.xlsx\",\n",
        "    \"Safawi   13-01-1964  ---  31-12-2023.xlsx\",\n",
        "    \"irbid  01-07-1978  ---  30-05-2024.xlsx\"\n",
        "    ]\n",
        "col_names = []\n",
        "datasets = []\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/Grad project/Data/\"\n",
        "for file in files:\n",
        "  datasets.append(read_data(path + file))\n",
        "for dataset in datasets:\n",
        "  col_names.append(dataset.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmpjSS2X8GFM",
        "outputId": "6a0ea7e7-4655-4806-cb1a-3e2188822822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-08957a7984c6>:8: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  final_df.index = pd.Series(final_df.index).fillna(method='ffill').values\n",
            "<ipython-input-3-08957a7984c6>:8: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  final_df.index = pd.Series(final_df.index).fillna(method='ffill').values\n",
            "<ipython-input-3-08957a7984c6>:8: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  final_df.index = pd.Series(final_df.index).fillna(method='ffill').values\n",
            "<ipython-input-3-08957a7984c6>:8: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  final_df.index = pd.Series(final_df.index).fillna(method='ffill').values\n",
            "<ipython-input-3-08957a7984c6>:8: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  final_df.index = pd.Series(final_df.index).fillna(method='ffill').values\n",
            "<ipython-input-3-08957a7984c6>:8: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  final_df.index = pd.Series(final_df.index).fillna(method='ffill').values\n",
            "<ipython-input-3-08957a7984c6>:8: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  final_df.index = pd.Series(final_df.index).fillna(method='ffill').values\n",
            "<ipython-input-3-08957a7984c6>:8: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  final_df.index = pd.Series(final_df.index).fillna(method='ffill').values\n"
          ]
        }
      ],
      "source": [
        "clean_datasets = [clean_data(df) for df in datasets]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvaObxwaAUDw",
        "outputId": "948e8577-8f8f-4e51-cd90-f69da9acfa8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 279222 entries, 1961-07-01 06:00:00 to 2024-05-30 21:00:00\n",
            "Data columns (total 10 columns):\n",
            " #   Column                       Non-Null Count   Dtype  \n",
            "---  ------                       --------------   -----  \n",
            " 0   Air Dew Point                279222 non-null  float64\n",
            " 1   Air Temperature (OC)         279222 non-null  float64\n",
            " 2   Humidity %                   279222 non-null  float64\n",
            " 3   Atmospheric Pressure         279222 non-null  float64\n",
            " 4   Liquid Precipitation         279222 non-null  float64\n",
            " 5   Cloud Cover %                279222 non-null  float64\n",
            " 6   Snow Depth                   279222 non-null  float64\n",
            " 7   Horizontal Visibility In m.  279222 non-null  float64\n",
            " 8   Wind Direction (Degrees)     279222 non-null  float64\n",
            " 9   Wind Speed (MPS)             279222 non-null  float64\n",
            "dtypes: float64(10)\n",
            "memory usage: 23.4 MB\n"
          ]
        }
      ],
      "source": [
        "clean_datasets[0].info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdXKLC69AW7n"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import log_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = clean_datasets[6].drop(columns = ['Snow Depth', 'Horizontal Visibility In m.'])"
      ],
      "metadata": {
        "id": "c2dc9fw1Z9PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Define target creation function\n",
        "def create_targets(df):\n",
        "    targets = (df['Liquid Precipitation'].shift(-1) > 0).astype(int).values[:-1]\n",
        "    return targets\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(df, window_size=8):\n",
        "    X = []\n",
        "    if df.shape[0] < window_size + 1:\n",
        "        print(\"Not enough data to create sequences\")\n",
        "        return None\n",
        "\n",
        "    for i in range(len(df) - window_size - 1):\n",
        "        seq = df.iloc[i:i+window_size].values\n",
        "        if seq.shape[0] == window_size:\n",
        "            X.append(seq)\n",
        "\n",
        "    return np.array(X, dtype=np.float32) if X else None\n",
        "\n",
        "# Prepare input-output data\n",
        "X = create_sequences(df)\n",
        "y = create_targets(df)\n",
        "y = y[:len(X)]  # Ensure matching lengths\n",
        "\n",
        "# Balance the dataset using SMOTE\n",
        "sm = SMOTE(sampling_strategy=0.5, random_state=42)  # Balances minority class to 50%\n",
        "X_resampled, y_resampled = sm.fit_resample(X.reshape(X.shape[0], -1), y)\n",
        "X_resampled = X_resampled.reshape(X_resampled.shape[0], X.shape[1], X.shape[2])\n",
        "\n",
        "# Time-based train-test split\n",
        "split_index = int(len(X_resampled) * 0.8)\n",
        "X_train, X_test = X_resampled[:split_index], X_resampled[split_index:]\n",
        "y_train, y_test = y_resampled[:split_index], y_resampled[split_index:]\n",
        "\n",
        "# Compute class weights\n",
        "y_train_flat = y_train.flatten()\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=y_train_flat)\n",
        "weight_for_0, weight_for_1 = class_weights\n",
        "\n",
        "# Define focal loss function\n",
        "def focal_loss(alpha=0.25, gamma=2.0):\n",
        "    def loss(y_true, y_pred):\n",
        "        bce = K.binary_crossentropy(y_true, y_pred)\n",
        "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
        "        loss = alpha * K.pow((1 - p_t), gamma) * bce\n",
        "        return K.mean(loss)\n",
        "    return loss\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Only 1 output for next-day prediction\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005), loss=focal_loss(), metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train, epochs=10, batch_size=128,\n",
        "    validation_data=(X_test, y_test), callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "roc_auc = roc_auc_score(y_test.flatten(), y_pred.flatten())\n",
        "pr_auc = average_precision_score(y_test.flatten(), y_pred.flatten())\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}, PR-AUC: {pr_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBkl541fM31P",
        "outputId": "81b11f73-ade4-4c51-9dcb-89165b201d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1074/1074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 64ms/step - accuracy: 0.8799 - loss: 0.0187 - val_accuracy: 0.7346 - val_loss: 0.0447\n",
            "Epoch 2/10\n",
            "\u001b[1m1074/1074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 59ms/step - accuracy: 0.9146 - loss: 0.0136 - val_accuracy: 0.8025 - val_loss: 0.0340\n",
            "Epoch 3/10\n",
            "\u001b[1m1074/1074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 60ms/step - accuracy: 0.9427 - loss: 0.0098 - val_accuracy: 0.9512 - val_loss: 0.0091\n",
            "Epoch 4/10\n",
            "\u001b[1m1074/1074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9582 - loss: 0.0073 - val_accuracy: 0.9516 - val_loss: 0.0084\n",
            "Epoch 5/10\n",
            "\u001b[1m1074/1074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 61ms/step - accuracy: 0.9677 - loss: 0.0058 - val_accuracy: 0.9337 - val_loss: 0.0096\n",
            "Epoch 6/10\n",
            "\u001b[1m1074/1074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 62ms/step - accuracy: 0.9734 - loss: 0.0048 - val_accuracy: 0.9312 - val_loss: 0.0100\n",
            "Epoch 7/10\n",
            "\u001b[1m1074/1074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 60ms/step - accuracy: 0.9747 - loss: 0.0047 - val_accuracy: 0.9543 - val_loss: 0.0072\n",
            "Epoch 8/10\n",
            "\u001b[1m1074/1074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 64ms/step - accuracy: 0.9806 - loss: 0.0036 - val_accuracy: 0.9448 - val_loss: 0.0098\n",
            "Epoch 9/10\n",
            "\u001b[1m1074/1074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 66ms/step - accuracy: 0.9824 - loss: 0.0034 - val_accuracy: 0.9905 - val_loss: 0.0020\n",
            "Epoch 10/10\n",
            "\u001b[1m1074/1074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 64ms/step - accuracy: 0.9830 - loss: 0.0032 - val_accuracy: 0.9861 - val_loss: 0.0023\n",
            "\u001b[1m1074/1074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step\n",
            "ROC-AUC: nan, PR-AUC: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming y_test contains true labels and y_pred contains predictions (thresholded at 0.5)\n",
        "y_pred_binary = (y_pred >= 0.5).astype(int)  # Convert probabilities to binary values\n",
        "\n",
        "# Compute precision, recall, and F1-score for each day separately\n",
        "precision = precision_score(y_test, y_pred_binary, average=None)  # Per day\n",
        "recall = recall_score(y_test, y_pred_binary, average=None)  # Per day\n",
        "f1 = f1_score(y_test, y_pred_binary, average=None)  # Per day\n",
        "\n",
        "# Compute overall (macro) precision, recall, and F1-score\n",
        "precision_macro = precision_score(y_test, y_pred_binary, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred_binary, average='macro')\n",
        "f1_macro = f1_score(y_test, y_pred_binary, average='macro')\n",
        "\n",
        "# Print results\n",
        "print(\"Precision per day:\", precision)\n",
        "print(\"Recall per day:\", recall)\n",
        "print(\"F1-score per day:\", f1)\n",
        "print(\"\\nOverall Macro Scores:\")\n",
        "print(\"Precision:\", precision_macro)\n",
        "print(\"Recall:\", recall_macro)\n",
        "print(\"F1-score:\", f1_macro)"
      ],
      "metadata": {
        "id": "Oz3I6hZZNH7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdbeab0e-c2b2-461e-a266-5b76662ada1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision per day: [0. 1.]\n",
            "Recall per day: [0.        0.9904834]\n",
            "F1-score per day: [0.         0.99521895]\n",
            "\n",
            "Overall Macro Scores:\n",
            "Precision: 0.5\n",
            "Recall: 0.4952416984371817\n",
            "F1-score: 0.49760947437678194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the timestamp you want to test\n",
        "test_timestamp = \"2022-11-14 03:00:00\"\n",
        "\n",
        "# Ensure the DataFrame has a DateTime index\n",
        "df.index = pd.to_datetime(df.index)\n",
        "\n",
        "# Locate the index of the given timestamp\n",
        "if test_timestamp in df.index:\n",
        "    rainy_day_index = df.index.get_loc(test_timestamp)\n",
        "\n",
        "    # Check if we have enough past data (8 days)\n",
        "    if rainy_day_index >= 8:\n",
        "        # Extract the input sequence (8 days before the test timestamp)\n",
        "        sample_input = df.iloc[rainy_day_index - 8:rainy_day_index].values\n",
        "\n",
        "        # Ensure shape matches model input\n",
        "        sample_input = np.expand_dims(sample_input, axis=0)  # Reshape to (1, 8, num_features)\n",
        "\n",
        "        # Make prediction\n",
        "        predicted_rain_probability = model.predict(sample_input)[0, 0]  # Get first prediction value\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Predicted probability of rain for {test_timestamp}: {predicted_rain_probability:.4f}\")\n",
        "\n",
        "        # Interpret results\n",
        "        threshold = 0.5  # You can adjust this threshold\n",
        "        predicted_rain = 1 if predicted_rain_probability >= threshold else 0\n",
        "        print(f\"Predicted rain for {test_timestamp}: {'Yes' if predicted_rain else 'No'}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Not enough past data to create a sequence before {test_timestamp}.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Timestamp {test_timestamp} not found in DataFrame index.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUHUEWlYMNbm",
        "outputId": "85fcdf41-fc2d-4b60-8a18-a740443d72ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
            "Predicted probability of rain for 2022-11-14 03:00:00: 0.0640\n",
            "Predicted rain for 2022-11-14 03:00:00: No\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWjP9Ik-MtBv",
        "outputId": "314afcb6-e17b-49dc-b212-6c3818cd5296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Abdelrahman\"\n",
        "!git config --global user.email \"aboud0322@gmail.com\""
      ],
      "metadata": {
        "id": "uwZXCA8XNzD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://github.com/abood-shehade/Weather-prediction"
      ],
      "metadata": {
        "id": "bBGbO7vYMuHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "WLZjZTg1NH7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"first commit\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxw9-MPJNMFC",
        "outputId": "0284048d-950c-4e39-d0b1-26bb5d138e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[master (root-commit) ee18717] first commit\n",
            " 21 files changed, 51025 insertions(+)\n",
            " create mode 100644 .config/.last_opt_in_prompt.yaml\n",
            " create mode 100644 .config/.last_survey_prompt.yaml\n",
            " create mode 100644 .config/.last_update_check.json\n",
            " create mode 100644 .config/active_config\n",
            " create mode 100644 .config/config_sentinel\n",
            " create mode 100644 .config/configurations/config_default\n",
            " create mode 100644 .config/default_configs.db\n",
            " create mode 100644 .config/gce\n",
            " create mode 100644 .config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db\n",
            " create mode 100644 .config/logs/2025.03.20/13.30.26.993509.log\n",
            " create mode 100644 .config/logs/2025.03.20/13.30.51.447492.log\n",
            " create mode 100644 .config/logs/2025.03.20/13.30.59.830488.log\n",
            " create mode 100644 .config/logs/2025.03.20/13.31.01.332468.log\n",
            " create mode 100644 .config/logs/2025.03.20/13.31.09.706011.log\n",
            " create mode 100644 .config/logs/2025.03.20/13.31.10.331550.log\n",
            " create mode 100755 sample_data/README.md\n",
            " create mode 100755 sample_data/anscombe.json\n",
            " create mode 100644 sample_data/california_housing_test.csv\n",
            " create mode 100644 sample_data/california_housing_train.csv\n",
            " create mode 100644 sample_data/mnist_test.csv\n",
            " create mode 100644 sample_data/mnist_train_small.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN5z11K9NmV4",
        "outputId": "160c11c5-23ef-4a0e-f0b9-e3673e935cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: The current branch master has no upstream branch.\n",
            "To push the current branch and set the remote as upstream, use\n",
            "\n",
            "    git push --set-upstream origin master\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote set-url origin https://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@github.com/abood-shehade/Weather-prediction.git\n",
        "\n",
        "\n",
        "\n",
        "!git push --set-upstream origin master\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIoDPJgfOSct",
        "outputId": "9209d693-dc16-4328-bee5-456e0455dd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 28, done.\n",
            "Counting objects:   3% (1/28)\rCounting objects:   7% (2/28)\rCounting objects:  10% (3/28)\rCounting objects:  14% (4/28)\rCounting objects:  17% (5/28)\rCounting objects:  21% (6/28)\rCounting objects:  25% (7/28)\rCounting objects:  28% (8/28)\rCounting objects:  32% (9/28)\rCounting objects:  35% (10/28)\rCounting objects:  39% (11/28)\rCounting objects:  42% (12/28)\rCounting objects:  46% (13/28)\rCounting objects:  50% (14/28)\rCounting objects:  53% (15/28)\rCounting objects:  57% (16/28)\rCounting objects:  60% (17/28)\rCounting objects:  64% (18/28)\rCounting objects:  67% (19/28)\rCounting objects:  71% (20/28)\rCounting objects:  75% (21/28)\rCounting objects:  78% (22/28)\rCounting objects:  82% (23/28)\rCounting objects:  85% (24/28)\rCounting objects:  89% (25/28)\rCounting objects:  92% (26/28)\rCounting objects:  96% (27/28)\rCounting objects: 100% (28/28)\rCounting objects: 100% (28/28), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (21/21), done.\n",
            "Writing objects: 100% (28/28), 8.42 MiB | 1.66 MiB/s, done.\n",
            "Total 28 (delta 5), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (5/5), done.\u001b[K\n",
            "To https://github.com/abood-shehade/Weather-prediction.git\n",
            " * [new branch]      master -> master\n",
            "Branch 'master' set up to track remote branch 'master' from 'origin'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v1pMm2aCOeE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}